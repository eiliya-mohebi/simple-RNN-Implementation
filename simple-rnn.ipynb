{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN Implementation\n",
    "\n",
    "This implementation of a Simple RNN follows the basic structure of recurrent neural networks, designed to process sequential data.\n",
    "\n",
    "1. **Initialization**:  \n",
    "   - The weights for input-to-hidden, hidden-to-hidden, and hidden-to-output connections are initialized.\n",
    "   - Bias terms are added for the hidden and output layers. The weights are scaled appropriately to work well with the $\\tanh$ activation function.\n",
    "\n",
    "2. **Forward Pass**:  \n",
    "   - For each time step in the sequence:\n",
    "     - The hidden state is updated using the input, previous hidden state, and hidden weights.\n",
    "     - The output is calculated based on the current hidden state and the output weights.\n",
    "   - This process captures temporal dependencies in the sequence.\n",
    "\n",
    "3. **Backward Pass**:  \n",
    "   - Gradients of the loss function are computed using backpropagation through time (BPTT).\n",
    "   - Since each hidden state affects multiple outputs, gradients are accumulated across time steps.\n",
    "   - The weights and biases are updated using the accumulated gradients to minimize the loss.\n",
    "\n",
    "4. **Training Loop**:  \n",
    "   - The network is trained iteratively over multiple epochs, processing the input sequence through forward and backward passes and updating parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1970-01-01    60.0\n",
       "1970-01-02    52.0\n",
       "1970-01-03    52.0\n",
       "1970-01-04    53.0\n",
       "1970-01-05    52.0\n",
       "1970-01-06    50.0\n",
       "1970-01-07    52.0\n",
       "1970-01-08    56.0\n",
       "1970-01-09    54.0\n",
       "1970-01-10    57.0\n",
       "Name: tmax, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in our data, and fill missing values\n",
    "data = pd.read_csv(\"clean_weather.csv\", index_col=0)\n",
    "data = data.ffill()\n",
    "\n",
    "# Display a sequence of temperatures\n",
    "data[\"tmax\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence has 10 elements.  The first sequence element (at time step 0) is `60`.  The second sequence element (at time step 1) is `52`, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60., 52., 52., 53., 52., 50., 52., 56., 54., 57.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our sequence into a single row of data\n",
    "data[\"tmax\"].head(10).to_numpy()[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Forward Pass\n",
    "\n",
    "In this section, we will construct a complete forward pass for a Recurrent Neural Network (RNN). The process begins with the initialization of weights and biases, ensuring that bias terms are incorporated into both the hidden and output layers.\n",
    "\n",
    "To ensure compatibility with the $\\tanh$ activation function, the weights and biases will be appropriately scaled. Specifically:\n",
    "\n",
    "- **Input and hidden weights**: These are initialized with small values to prevent the $\\tanh$ nonlinearity from saturating, where it squashes inputs to either `1` or `-1`.\n",
    "- **Output weights**: These are initialized with larger values, compensating for the fact that the hidden layer outputs are constrained to the range `[-1, 1]`.\n",
    "\n",
    "While a complete RNN can learn optimal parameters through training, initializing weights and biases within these ranges facilitates the gradient descent process, improving convergence behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Define our weights and biases\n",
    "# Scale them down so values get through the tanh nonlinearity\n",
    "i_weight = np.random.rand(1,5) / 5 - .1\n",
    "h_weight = np.random.rand(5,5) / 5 - .1\n",
    "h_bias = np.random.rand(1,5) / 5 - .1\n",
    "\n",
    "# Tanh pushes values to between -1 and 1, so scale up the output weights\n",
    "o_weight = np.random.rand(5,1) * 50\n",
    "o_bias = np.random.rand(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we can write the forward pass as a for loop.  This loop will process sequence elements one by one.  We'll store the output prediction and the hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array to store the output predictions\n",
    "outputs = np.zeros(3)\n",
    "# An array to store hidden states for use in backpropagation\n",
    "hiddens = np.zeros((3, 5))\n",
    "\n",
    "# This will store the previous hidden state, since we'll need it to calculate the current hidden step\n",
    "prev_hidden = None\n",
    "sequence = data[\"tmax\"].tail(3).to_numpy()\n",
    "\n",
    "for i in range(3):\n",
    "    # Get the input sequence at the given position\n",
    "    x = sequence[i].reshape(1,1)\n",
    "\n",
    "    # Multiply input by input weight\n",
    "    xi = x @ i_weight\n",
    "    if prev_hidden is not None:\n",
    "        # Add previous hidden to input\n",
    "        xh = xi + prev_hidden @ h_weight + h_bias\n",
    "    else:\n",
    "        xh = xi\n",
    "\n",
    "    # Apply our activation function\n",
    "    xh = np.tanh(xh)\n",
    "    prev_hidden = xh\n",
    "    hiddens[i,] = xh\n",
    "\n",
    "    # Multiply by the output weight\n",
    "    xo = xh @ o_weight + o_bias\n",
    "    outputs[i] = xo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A lot of the above step is very similar to backpropagation in a dense neural network.  The main difference comes in the next sequence position (1) where we need to consider multiple gradients at the hidden step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_grad = loss_grad[1].reshape(1,1)\n",
    "\n",
    "o_weight_grad += hiddens[1][:,np.newaxis] @ l1_grad\n",
    "o_bias_grad += np.mean(l1_grad)\n",
    "\n",
    "h1_grad = l1_grad @ o_weight.T\n",
    "\n",
    "# We do have a next sequence position (2), so we need to include that gradient\n",
    "# We multiply the h2 gradient by the weight to pull it back to the current sequence position\n",
    "h1_grad += h2_grad @ h_weight.T\n",
    "\n",
    "# The rest of the operation is the same\n",
    "tanh_deriv = 1 - hiddens[1,:][np.newaxis,:] ** 2\n",
    "h1_grad = np.multiply(h1_grad, tanh_deriv)\n",
    "\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h1_grad\n",
    "h_bias_grad += np.mean(h1_grad)\n",
    "\n",
    "i_weight_grad += sequence[1].reshape(1,1).T @ h1_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we can do the final sequence position, 0.  The main difference here is that we don't update the hidden gradient, since there is no previous sequence position that gave us hidden state input in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0_grad = loss_grad[0].reshape(1,1)\n",
    "\n",
    "o_weight_grad += hiddens[0][:,np.newaxis] @ l0_grad\n",
    "o_bias_grad += np.mean(l0_grad)\n",
    "\n",
    "h0_grad = l0_grad @ o_weight.T\n",
    "\n",
    "h0_grad += h1_grad @ h_weight.T\n",
    "\n",
    "tanh_deriv = 1 - hiddens[0,:][np.newaxis,:] ** 2\n",
    "h0_grad = np.multiply(h0_grad, tanh_deriv)\n",
    "\n",
    "# We don't update the hidden weight, since there was no previous hidden state\n",
    "# We can update the hidden bias if you want\n",
    "\n",
    "i_weight_grad += sequence[0].reshape(1,1).T @ h0_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now look at our gradient updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-154.9774425 ,  -61.87971495,  -11.63213862,  -81.30882966,\n",
       "          17.37148576]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_weight_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "With the forward pass implemented, the next step is to consider the backward pass for updating the model parameters. The primary complexity in the backward pass arises from the fact that each parameter influences both the current output and future outputs.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Final Time Step**:  \n",
    "   At the last sequence item, the hidden state only impacts the output of the final time step. This simplifies the gradient computation for this specific step.\n",
    "\n",
    "2. **Intermediate Time Steps**:  \n",
    "   In contrast, the hidden state at an earlier time step (e.g., time step 2) contributes to:\n",
    "   - The output of the current time step.\n",
    "   - The hidden state of the subsequent time step, which, in turn, impacts all future outputs.\n",
    "\n",
    "### Implications for Backpropagation\n",
    "\n",
    "During backpropagation, this dependency means that some parameters influence multiple outputs across the sequence. Consequently, the gradients for these parameters must accumulate contributions from all relevant outputs. Properly aggregating these gradients ensures accurate updates to the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hidden = None\n",
    "\n",
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5\n",
    "\n",
    "for i in range(2, -1, -1):\n",
    "    l_grad = loss_grad[i].reshape(1,1)\n",
    "\n",
    "    o_weight_grad += hiddens[i][:,np.newaxis] @ l_grad\n",
    "    o_bias_grad += np.mean(l_grad)\n",
    "\n",
    "    o_grad = l_grad @ o_weight.T\n",
    "\n",
    "    # Only add in the hidden gradient if a next sequence exists\n",
    "    if next_hidden is not None:\n",
    "        h_grad = o_grad + next_hidden @ h_weight.T\n",
    "    else:\n",
    "        h_grad = o_grad\n",
    "\n",
    "    tanh_deriv = 1 - hiddens[i,:][np.newaxis,:] ** 2\n",
    "    h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "    next_hidden = h_grad\n",
    "\n",
    "    # Don't update the hidden weights for the first sequence position\n",
    "    if i > 0:\n",
    "        h_weight_grad += hiddens[i-1,:][:,np.newaxis] @ h_grad\n",
    "        h_bias_grad += np.mean(h_grad)\n",
    "\n",
    "    i_weight_grad += sequence[i].reshape(1,1).T @ h_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use gradient descent to make parameter updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "# We'll divide the learning rate by the sequence length, since we were adding together the gradients\n",
    "# This makes training the model more stable\n",
    "lr = lr / 3\n",
    "\n",
    "i_weight -= i_weight_grad * lr\n",
    "h_weight -= h_weight_grad * lr\n",
    "h_bias -= h_bias_grad * lr\n",
    "o_weight -= o_weight_grad * lr\n",
    "o_bias -= o_bias_grad * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Implementation of an RNN\n",
    "\n",
    "A full implementation of a Recurrent Neural Network (RNN) involves several critical steps, combining both the forward pass and backward pass to enable the network to process sequential data and learn meaningful patterns. Below, we outline the implementation process.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Initialization**\n",
    "Before running the network, we need to initialize its parameters:\n",
    "\n",
    "- **Weights and Biases**:\n",
    "  - **Input weights**: Small values to prevent saturation of the $\\tanh$ activation function.\n",
    "  - **Hidden weights**: Small values for stable hidden state updates.\n",
    "  - **Output weights**: Larger values to account for the smaller range of hidden states (`[-1, 1]`).\n",
    "  - **Bias terms**: Initialized to small values to ensure the model starts with a balanced activation.\n",
    "\n",
    "This initialization helps stabilize training by ensuring gradients are well-behaved at the beginning of training.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Forward Pass**\n",
    "\n",
    "The forward pass computes the outputs for the entire input sequence, step by step. This involves:\n",
    "\n",
    "1. **Input Processing**:\n",
    "   At each time step, the input vector is multiplied by the input weights.\n",
    "\n",
    "2. **Hidden State Update**:\n",
    "   The hidden state is computed using the formula:\n",
    "   \\[\n",
    "   h_t = \\tanh(W_{ih} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( h_t \\) is the current hidden state.\n",
    "   - \\( x_t \\) is the input at time \\( t \\).\n",
    "   - \\( W_{ih} \\) and \\( W_{hh} \\) are the input-to-hidden and hidden-to-hidden weights, respectively.\n",
    "   - \\( b_h \\) is the bias for the hidden layer.\n",
    "\n",
    "3. **Output Computation**:\n",
    "   The output at each time step is computed as:\n",
    "   \\[\n",
    "   y_t = W_{ho} \\cdot h_t + b_o\n",
    "   \\]\n",
    "   where \\( W_{ho} \\) is the hidden-to-output weight matrix, and \\( b_o \\) is the output bias.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Backward Pass**\n",
    "\n",
    "The backward pass adjusts the parameters based on the loss function by propagating the error gradients back through time. This process includes:\n",
    "\n",
    "1. **Loss Calculation**:\n",
    "   Compute the loss function (e.g., mean squared error or cross-entropy loss) based on the predicted outputs \\( y_t \\) and the true labels.\n",
    "\n",
    "2. **Gradient Computation**:\n",
    "   Gradients are computed using the chain rule, considering that:\n",
    "   - Parameters in the **final hidden state** affect only the last output.\n",
    "   - Parameters in **earlier hidden states** affect both the current output and all future outputs.\n",
    "\n",
    "3. **Gradient Accumulation**:\n",
    "   For parameters influencing multiple outputs (e.g., hidden-to-hidden weights), gradients from all affected outputs are accumulated.\n",
    "\n",
    "4. **Weight Updates**:\n",
    "   Using an optimization algorithm (e.g., stochastic gradient descent), weights and biases are updated:\n",
    "   \\[\n",
    "   \\theta \\gets \\theta - \\eta \\cdot \\nabla_\\theta\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( \\theta \\) represents the parameters.\n",
    "   - \\( \\eta \\) is the learning rate.\n",
    "   - \\( \\nabla_\\theta \\) is the gradient of the loss with respect to \\( \\theta \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Training Loop**\n",
    "\n",
    "The RNN is trained iteratively over multiple epochs. At each epoch:\n",
    "- The forward pass computes predictions for the entire sequence.\n",
    "- The backward pass computes gradients and updates the parameters.\n",
    "- The loss is monitored to ensure convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Challenges and Solutions**\n",
    "\n",
    "1. **Vanishing/Exploding Gradients**:\n",
    "   - Gradients may vanish or explode during backpropagation through time (BPTT). Solutions include:\n",
    "     - Gradient clipping to cap large gradients.\n",
    "     - Using gated architectures like LSTMs or GRUs.\n",
    "\n",
    "2. **Initialization Sensitivity**:\n",
    "   - Proper initialization (as discussed) prevents instability during training.\n",
    "\n",
    "3. **Sequence Length**:\n",
    "   - For very long sequences, truncated BPTT can be used to limit gradient propagation to a manageable window.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Inference**\n",
    "\n",
    "After training, the RNN can be used for inference. Given an input sequence, the forward pass computes the outputs, which can be used for tasks like:\n",
    "- Sequence-to-sequence translation.\n",
    "- Time-series forecasting.\n",
    "- Text generation.\n",
    "\n",
    "---\n",
    "\n",
    "By combining these steps, an RNN can be fully implemented to process sequential data, learn temporal dependencies, and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "# Define predictors and target\n",
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "# Scale our data to have mean 0\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "# Split into train, valid, test sets\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we can initialize our weights and biases.  We'll scale our parameters so they are relatively small.  This helps the network descend better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        np.random.seed(0)\n",
    "        k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we'll write a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    hiddens = []\n",
    "    outputs = []\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
    "        for j in range(x.shape[0]):\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
    "            # Activation.  tanh avoids outputs getting larger and larger.\n",
    "            hidden_x = np.tanh(hidden_x)\n",
    "            # Store hidden for use in backprop\n",
    "            hidden[j,:] = hidden_x\n",
    "\n",
    "            # Output layer\n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            output[j,:] = output_x\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "    return hiddens, outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And a backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = hiddens[i]\n",
    "        next_h_grad = None\n",
    "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):\n",
    "            # Add newaxis in the first dimension\n",
    "            out_grad = grad[j,:][np.newaxis, :]\n",
    "\n",
    "            # Output updates\n",
    "            # np.newaxis creates a size 1 axis, in this case transposing matrix\n",
    "            o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate gradient to hidden unit\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                # Add the gradients together to combine output contribution and hidden contribution\n",
    "                h_grad += hh_grad\n",
    "\n",
    "            # Pull the gradient across the current hidden nonlinearity\n",
    "            # derivative of tanh is 1 - tanh(x) ** 2\n",
    "            # So we take the output of tanh (next hidden state), and plug in\n",
    "            tanh_deriv = 1 - hidden[j][np.newaxis,:] ** 2\n",
    "\n",
    "            # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
    "            # Effect is to pull value across nonlinearity\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            # Store to compute h grad for previous sequence position\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            # If we're not at the very beginning\n",
    "            if j > 0:\n",
    "                # Multiply input from previous layer by post-nonlinearity grad at current layer\n",
    "                h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
    "\n",
    "        # Normalize lr by number of sequence elements\n",
    "        lr = lr / x.shape[0]\n",
    "        i_weight -= i_weight_grad * lr\n",
    "        h_weight -= h_weight_grad * lr\n",
    "        h_bias -= h_bias_grad * lr\n",
    "        o_weight -= o_weight_grad * lr\n",
    "        o_bias -= o_bias_grad * lr\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can end by setting up a training loop and measuring error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss 3122.594400144508 valid loss 2171.3186862102025\n",
      "Epoch: 50 train loss 30.593193275313595 valid loss 30.568271740103427\n",
      "Epoch: 100 train loss 25.263986813543738 valid loss 24.435517510355645\n",
      "Epoch: 150 train loss 22.9567624295313 valid loss 22.177010971976852\n",
      "Epoch: 200 train loss 22.306774327704215 valid loss 21.557992202834164\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "lr = 1e-5\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": 3},\n",
    "    {\"type\": \"rnn\", \"hidden\": 4, \"output\": 1}\n",
    "]\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sequence_len = 7\n",
    "    epoch_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j:(j+sequence_len),]\n",
    "        seq_y = train_y[j:(j+sequence_len),]\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = mse_grad(seq_y, outputs)\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        sequence_len = 7\n",
    "        valid_loss = 0\n",
    "        for j in range(valid_x.shape[0] - sequence_len):\n",
    "            seq_x = valid_x[j:(j+sequence_len),]\n",
    "            seq_y = valid_y[j:(j+sequence_len),]\n",
    "            _, outputs = forward(seq_x, layers)\n",
    "            valid_loss += mse(seq_y, outputs)\n",
    "\n",
    "        print(f\"Epoch: {epoch} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
